"""retrain_service.py
ëª¨ë¸ ì¬í•™ìŠµ Â· ë²„ì €ë‹ Â· S3 ì—…ë¡œë“œ ì„œë¹„ìŠ¤
-------------------------------------------------
Â· ë°ì´í„° ì ì¬/ì „ì²˜ë¦¬ :  app.services.data_service
Â· ê³µí†µ ìƒìˆ˜           :  app.core.constants
Â· ëŸ°íƒ€ì„ ì„¤ì •         :  app.core.config.settings

Usage
-----
from app.services import retrain_service
result = retrain_service.train_and_upload()
print(result)
"""

from __future__ import annotations

import io
import json
import logging
from datetime import datetime, timezone, timedelta
from typing import List, Optional, Tuple

import boto3
import lightgbm as lgb
import numpy as np
import pandas as pd
from sklearn.metrics import (mean_absolute_error, mean_squared_error,
                             r2_score)
from sklearn.model_selection import train_test_split

from app.core.config import settings
from app.core import constants
from app.service import data_service
from app.core.logging_config import get_logger

from app.core import constants as C
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë¡œê¹… ì„¤ì •
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
logger = get_logger("monitory.retrain")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# S3 helpers
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _get_s3_client():
    return boto3.client(
        "s3",
        region_name=settings.aws_region,
        aws_access_key_id=settings.aws_access_key_id,
        aws_secret_access_key=settings.aws_secret_access_key,
    )


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Raw ë°ì´í„° ì ì¬
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _list_object_keys(bucket: str, prefix: str) -> List[str]:
    s3 = _get_s3_client()
    logger.debug("ğŸ” S3 list prefix=%s", prefix)
    paginator = s3.get_paginator("list_objects_v2")
    keys: List[str] = []
    for page in paginator.paginate(Bucket=bucket, Prefix=prefix):
        for obj in page.get("Contents", []):
            key = obj["Key"]
            if key.endswith(".json"):
                keys.append(key)
    logger.info("ğŸ—‚ï¸  %sê°œ JSON (bucket=%s, prefix=%s)", len(keys), bucket, prefix)
    return keys


def _load_ndjson(keys: List[str], bucket: str, sample_n: Optional[int] = None) -> pd.DataFrame:
    s3 = _get_s3_client()
    dfs: List[pd.DataFrame] = []
    iterable = keys if sample_n is None else keys[:sample_n]
    for k in iterable:
        body = s3.get_object(Bucket=bucket, Key=k)["Body"].read()
        dfs.append(pd.read_json(io.BytesIO(body), lines=True))
    df = pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()
    logger.info("ğŸ“¥ concat â†’ %s rows", len(df))
    return df

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Raw âœ df_wide + ì „ì²˜ë¦¬ & RUL (Colab ë¡œì§)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_DOWNSTREAM_WIN = 5
_MAX_RUL = 30


def _prepare_training_df(df_raw: pd.DataFrame, win: int = _DOWNSTREAM_WIN) -> pd.DataFrame:
    """Raw NDJSON â†’ wide + rolling + RUL ê³„ì‚°"""
    if df_raw.empty:
        logger.error("raw_df empty â†’ ì „ì²˜ë¦¬ ì¤‘ë‹¨")
        return pd.DataFrame()

    logger.info("ğŸ”° raw rows=%s", len(df_raw))
    df = df_raw.copy()

    # timestamp íŒŒì‹± & ë¶ˆí•„ìš” col ì œê±°
    df["timestamp"] = pd.to_datetime(df["time"], utc=True)
    df.drop(columns=["time", "zoneId", "sensorId"], inplace=True, errors="ignore")

    # ì„¼ì„œ í•„í„°ë§
    keep = [
        "active_power", "humid", "pressure", "reactive_power", "temp", "vibration"
    ]
    df_use = df[df["sensorType"].isin(keep)].copy()

    # 1ì‹œê°„ floor
    df_use["ts_hour"] = df_use["timestamp"].dt.floor("h")

    # pivot
    df_wide = (
        df_use.pivot_table(
            index=["ts_hour", "equipId"],
            columns="sensorType",
            values="val",
            aggfunc="max",
        )
        .reset_index()
        .rename(columns={"ts_hour": "timestamp"})
    )

    # rolling feature
    num_cols = [
        "temp", "pressure", "vibration", "humid", "active_power", "reactive_power"
    ]
    for col in num_cols:
        grp = df_wide.groupby("equipId")[col]
        df_wide[f"{col}_rollmean"] = grp.rolling(win, 1).mean().reset_index(level=0, drop=True)
        df_wide[f"{col}_rollstd"] = grp.rolling(win, 1).std().reset_index(level=0, drop=True).fillna(0)

    df_wide["power_factor"] = (
        df_wide["active_power"] / np.sqrt(df_wide["active_power"] ** 2 + df_wide["reactive_power"] ** 2)
    ).fillna(0)

    # ê²°ì¸¡ ë³´ì •
    raw_cols = ["active_power", "reactive_power", "temp", "pressure", "vibration", "humid"]
    roll_std = [c for c in df_wide.columns if c.endswith("_rollstd")]

    df_wide = df_wide.sort_values(["equipId", "timestamp"])
    df_wide[raw_cols] = df_wide.groupby("equipId")[raw_cols].ffill(limit = 3).bfill(limit = 1)
    for c in raw_cols:
        df_wide[c] = df_wide[c].fillna(df_wide[c].median())
    df_wide[roll_std] = df_wide[roll_std].fillna(0)

    # faulty & RUL
    df_use["alert"] = 0
    for sensor, (lo, hi) in constants.ALERT_THRESH.items():
        m = df_use["sensorType"] == sensor
        vals = df_use.loc[m, "val"]
        df_use.loc[m, "alert"] = ((vals < lo) | (vals > hi)).astype("int8")

    faulty = (
        df_use.groupby(["ts_hour", "equipId"])["alert"].sum().reset_index(name="cnt")
    )
    faulty["faulty"] = (faulty["cnt"] >= 2).astype(int)

    df_wide = df_wide.merge(
        faulty.rename(columns={"ts_hour": "timestamp"})[["timestamp", "equipId", "faulty"]],
        on=["timestamp", "equipId"],
        how="left",
    ).fillna({"faulty": 0}).astype({"faulty": "int8"})

    def _add_rul(g: pd.DataFrame) -> pd.DataFrame:
        n = len(g)
        rul = np.full(n, np.nan, dtype=np.float32)
        nxt: int | None = None
        for i in range(n - 1, -1, -1):
            if g.iloc[i]["faulty"] == 1:
                nxt = 0
            else:
                nxt = (nxt + 1) if nxt is not None else np.nan
            rul[i] = nxt
        g["rul"] = rul
        return g

    df_wide = df_wide.sort_values(["equipId", "timestamp"]).groupby("equipId", group_keys=False).apply(_add_rul)
    df_wide["rul"] = df_wide["rul"].fillna(_MAX_RUL).clip(upper=_MAX_RUL)

    # â”€â”€ ì»¬ëŸ¼ëª… ì •ê·œí™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 1) equipId â†’ equipment
    if "equipId" in df_wide.columns:
        df_wide = df_wide.rename(columns={"equipId": "equipment"})
        logger.debug("ğŸ”§ rename equipId â†’ equipment ì™„ë£Œ")

        # 2) ì„¼ì„œëª… í‘œì¤€í™”  (tempâ†’temperature, humidâ†’humidity)
    sensor_map = {"temp": "temperature", "humid": "humidity"}
    rename_dict = {}

    for old, new in sensor_map.items():
        # base ì»¬ëŸ¼
        if old in df_wide.columns:
            rename_dict[old] = new
        # rolling mean / std
        if f"{old}_rollmean" in df_wide.columns:
            rename_dict[f"{old}_rollmean"] = f"{new}_rollmean"
        if f"{old}_rollstd" in df_wide.columns:
            rename_dict[f"{old}_rollstd"] = f"{new}_rollstd"

    if rename_dict:
        df_wide = df_wide.rename(columns=rename_dict)
        logger.debug("ğŸ”§ sensor ì»¬ëŸ¼ëª… í‘œì¤€í™” ì™„ë£Œ: %s", rename_dict)

    logger.info("âœ… ì „ì²˜ë¦¬ ì™„ë£Œ â†’ shape=%s", df_wide.shape)
    return df_wide

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë°ì´í„° ë°¸ëŸ°ì‹± (Colab ë¡œì§ ê·¸ëŒ€ë¡œ)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DOWN_RATIO_ZERO = C.DOWN_RATIO_ZERO
OVER_RATIO      = C.OVER_RATIO

def _balance_rul(df: pd.DataFrame) -> pd.DataFrame:
    zero_df = df[df.rul == 0].sample(frac=DOWN_RATIO_ZERO, random_state=42)
    dfs = [zero_df]
    for k, v in OVER_RATIO.items():
        tmp = df[df.rul == k]
        if not tmp.empty:
            dfs.append(pd.concat([tmp] * v, ignore_index=True))
    balanced = pd.concat(dfs, ignore_index=True).sample(frac=1, random_state=42)
    logger.info("âš–ï¸  balance â†’ %s rows (down 0, over 1â€’15)", len(balanced))
    return balanced


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í•™ìŠµ ë¡œì§
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_FEATURE_COLS = constants.FEATURE_COLS
_TARGET_COL = "rul"


def _train_model(df: pd.DataFrame) -> Tuple[lgb.LGBMRegressor, dict]:
    df["equipment"] = df["equipment"].astype("category")
    X = df[_FEATURE_COLS]
    y = df[_TARGET_COL]

    X_temp, X_test, y_temp, y_test = train_test_split(
        X, y, test_size=0.20, random_state=42, stratify=y
    )
    X_train, X_valid, y_train, y_valid = train_test_split(
        X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp
    )

    model = lgb.LGBMRegressor(
        n_estimators=600,
        learning_rate=0.05,
        num_leaves=64,
        objective="regression",
        random_state=42,
        verbosity = -1,
    )

    logger.info("ğŸš€ LightGBM fit ì‹œì‘ (train=%s, valid=%s)", len(X_train), len(X_valid))
    model.fit(
        X_train,
        y_train,
        eval_set=[(X_train, y_train), (X_valid, y_valid)],
        eval_names=["train", "valid"],
        eval_metric="rmse",
        categorical_feature=["equipment"],
        callbacks=[
            lgb.log_evaluation(period=0),
            lgb.early_stopping(50),
            lgb.reset_parameter({"verbose": -1})  # â† í•µì‹¬ ìˆ˜ì •
        ],
    )

    y_pred = model.predict(X_test)
    mse = mean_squared_error(y_test, y_pred)
    rmse = float(np.sqrt(mse))
    mae = float(mean_absolute_error(y_test, y_pred))
    r2 = float(r2_score(y_test, y_pred))
    logger.info("âœ… New Eval | RMSE=%.3f MAE=%.3f R2=%.4f", rmse, mae, r2)

    return model, {"rmse": rmse, "mae": mae, "r2": r2}


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ëª¨ë¸ ì €ì¥ & ë²„ì „ ê´€ë¦¬
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

_MODEL_BUCKET = settings.s3_model_bucket
_LATEST_MODEL_KEY = "models/latest/lgbm_regressor.json"
_LATEST_METRIC_KEY = "models/latest/metrics.json"
_VERSION_TEMPLATE = "models/{:%Y/%m/%d/%H%M%S}/{}"  # dt, filename


def _fetch_latest_rmse() -> Tuple[float, dict]:
    s3 = _get_s3_client()
    try:
        meta = json.loads(s3.get_object(Bucket=_MODEL_BUCKET, Key=_LATEST_METRIC_KEY)["Body"].read())
        return meta.get("rmse", float("inf")), meta
    except s3.exceptions.NoSuchKey:
        logger.warning("ğŸ†• ìµœì´ˆ í•™ìŠµ (latest ë©”íŠ¸ë¦­ ì—†ìŒ)")
        return float("inf"), {}
    except Exception as e:
        logger.error("ë©”íŠ¸ë¦­ ë¡œë“œ ì‹¤íŒ¨: %s", e)
        return float("inf"), {}


def _upload(version_key: str, model_dict: dict, metrics: dict, promote: bool):
    s3 = _get_s3_client()
    # ë²„ì „ íˆìŠ¤í† ë¦¬ ì €ì¥
    s3.put_object(Bucket=_MODEL_BUCKET, Key=version_key + "/lgbm_regressor.json", Body=json.dumps(model_dict).encode())
    s3.put_object(Bucket=_MODEL_BUCKET, Key=version_key + "/metrics.json", Body=json.dumps(metrics).encode())

    if promote:
        s3.put_object(Bucket=_MODEL_BUCKET, Key=_LATEST_MODEL_KEY, Body=json.dumps(model_dict).encode())
        s3.put_object(Bucket=_MODEL_BUCKET, Key=_LATEST_METRIC_KEY, Body=json.dumps(metrics).encode())
        logger.info("ğŸ† ìµœì‹  ëª¨ë¸ ìŠ¹ê²© â†’ %s", _LATEST_MODEL_KEY)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Public ì—”íŠ¸ë¦¬í¬ì¸íŠ¸ [ìˆ˜ì •] ë‚ ì§œ ë²”ìœ„ ì§€ì›
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
MIN_BALANCED_ROWS = C.MIN_BALANCED_ROWS      # ì›í•˜ëŠ” ìµœì†Œ í–‰ ìˆ˜

def train_and_upload(
    start_day: str | None = None,   # YYYY-MM-DD
    end_day:   str | None = None,   # YYYY-MM-DD
    sample_n:  int  | None = None   # NDJSON ì¼ë¶€ë§Œ ì½ê³  ì‹¶ì„ ë•Œ
) -> dict:
    """
    ë‚ ì§œ ë²”ìœ„ë¥¼ ë„˜ê¸°ë©´ day-prefix ëª¨ë“œ,
    ë‘˜ ë‹¤ None ì´ë©´ ì›”-prefix ëª¨ë“œ(ê¸°ì¡´ ë°©ì‹).
    """

    bucket = settings.s3_input_data_bucket
    keys: list[str] = []

    # â‘  ì¼ì ë²”ìœ„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if start_day and end_day:
        day = datetime.strptime(start_day, "%Y-%m-%d")
        end = datetime.strptime(end_day, "%Y-%m-%d")
        while day <= end:
            p = f"EQUIPMENT/date={day.strftime('%Y-%m-%d')}"
            keys.extend(_list_object_keys(bucket, p))
            day += timedelta(days=1)

        # â‘¡ ì›” ë‹¨ìœ„(ë ˆê±°ì‹œ) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    else:
        # ì•„ë¬´ íŒŒë¼ë¯¸í„°ë„ ì—†ìœ¼ë©´ â€œì˜¤ëŠ˜â€ ê¸°ì¤€ ì›”
        month_key = (start_day or end_day or datetime.utcnow().strftime("%Y-%m"))[:7]
        p = f"EQUIPMENT/date={month_key}"
        keys = _list_object_keys(bucket, p)

    if not keys:
        msg = f"S3 ë°ì´í„° ì—†ìŒ (prefix={p})"
        logger.error(msg)
        return {"status": "error", "msg": msg}

    logger.info("ğŸ”¢ ì´ keys=%s (S3 objects to load)", len(keys))

    raw_df = _load_ndjson(keys, bucket, sample_n)
    if raw_df.empty:
        msg = "ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨ (empty df)"
        logger.error(msg)
        return {"status": "error", "msg": msg}

    # 2) ì „ì²˜ë¦¬ & RUL -----------------------------------------------------------------
    df_wide = _prepare_training_df(raw_df)
    if df_wide is None or df_wide.empty:
        msg = "ì „ì²˜ë¦¬ ì‹¤íŒ¨"
        logger.error(msg)
        return {"status": "error", "msg": msg}

    balanced_df = _balance_rul(df_wide)

    if len(balanced_df) < MIN_BALANCED_ROWS:
        msg = f"balanced rows {len(balanced_df)} < {MIN_BALANCED_ROWS}"
        logger.warning("â­ï¸  %s â†’ ì¬í•™ìŠµ Skip", msg)
        return {"status": "skip", "reason": "too_few_rows", "rows": len(balanced_df)}

    # 3) í•™ìŠµ -------------------------------------------------------------------------
    try:
        model, metrics = _train_model(balanced_df)
    except Exception as e:
        logger.exception("ğŸ’¥ Train failed: %s", e)
        return {"status": "error", "reason": "train_failed", "msg": str(e)}

    # 4) ë©”íŠ¸ë¦­ ë¹„êµ & ì €ì¥ ------------------------------------------------------------
    MIN_R2 = 0.20  # ê¸°ì¤€ì¹˜, config/constant ë¡œ ë¹¼ë‘ê¸°
    old_rmse, old_meta = _fetch_latest_rmse()

    promote = (metrics["rmse"] < old_rmse) and (metrics["r2"] >= MIN_R2)

    # ë²„ì „ ê²½ë¡œ (ë””ë ‰í„°ë¦¬)
    now = datetime.utcnow().replace(tzinfo=timezone.utc)
    version_dir = _VERSION_TEMPLATE.format(now, "").rstrip("/")


    logger.info("âœ… Old Eval | RMSE=%.3f ", old_rmse)
    _upload(version_dir, model.booster_.dump_model(), metrics, promote)

    logger.info("ğŸ“¤ S3 ì—…ë¡œë“œ ì™„ë£Œ | promote=%s | version_dir=%s", promote, version_dir)

    return {
        "status": "ok",
        "trained_on": len(balanced_df),
        "metrics": metrics,
        "promoted": promote,
        "version_dir": version_dir,
        "prev_rmse": old_rmse if old_rmse != float("inf") else None,
    }